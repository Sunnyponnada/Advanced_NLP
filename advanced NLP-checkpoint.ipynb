{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2eb09de-b42a-4f17-967b-8e9cdda9e01e",
   "metadata": {},
   "source": [
    "# Assignment 2: Extracting Topics from the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9645325-8b1f-4b42-aa97-fedaa84e7498",
   "metadata": {},
   "source": [
    "## Objective\n",
    "the fundamentals of topic modeling, preprocessing text for topic modeling, and evaluating the generated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4ae61-c2f9-4e88-9924-b8b77d636810",
   "metadata": {},
   "source": [
    "###  Task 1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8309fbd-215c-45d5-a782-7f0654abf90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The stock market has been experiencing volatil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The economy is growing, and businesses are opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Climate change is a critical issue that needs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Advances in artificial intelligence have revol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The rise of electric vehicles is shaping the f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                               text\n",
       "0            1  The stock market has been experiencing volatil...\n",
       "1            2  The economy is growing, and businesses are opt...\n",
       "2            3  Climate change is a critical issue that needs ...\n",
       "3            4  Advances in artificial intelligence have revol...\n",
       "4            5  The rise of electric vehicles is shaping the f..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# Load the dataset\n",
    "df = pd.read_csv('text_docs - text_docs.csv')\n",
    "\n",
    "# Display the total rows and first few rows\n",
    "print(f\"Total Rows: {df.shape[0]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1c3168-9384-4b1e-a2a0-fb11ac5cb5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Documents: 10\n",
      "document_id    0\n",
      "text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for unique documents\n",
    "unique_docs = df['text'].nunique()\n",
    "print(f\"Total Unique Documents: {unique_docs}\")\n",
    "\n",
    "# Check for null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f253538-173c-48eb-867f-4c602d537f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  The stock market has been experiencing volatil...   \n",
      "1  The economy is growing, and businesses are opt...   \n",
      "2  Climate change is a critical issue that needs ...   \n",
      "3  Advances in artificial intelligence have revol...   \n",
      "4  The rise of electric vehicles is shaping the f...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  stock market experiencing volatility due econo...  \n",
      "1       economy growing businesses optimistic future  \n",
      "2  climate change critical issue needs immediate ...  \n",
      "3  advances artificial intelligence revolutionize...  \n",
      "4  rise electric vehicles shaping future automobi...  \n"
     ]
    }
   ],
   "source": [
    "# Step 3: Identify Preprocessing Steps\n",
    "# Load the stop words for English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of cleaned text\n",
    "print(df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fe274-921d-48be-9b33-8479c07bc56a",
   "metadata": {},
   "source": [
    "1. **Remove Stop Words:** Common words like \"the,\" \"and,\" etc., which may dilute topic modeling.\n",
    "2. **Lowercase Conversion:** Standardize text for uniformity.\n",
    "3. **Tokenization:** Split sentences into individual words.\n",
    "4. **Remove Special Characters:** Exclude punctuation or numeric values.\n",
    "5. **Lemmatization:** Reduce words to their base or root form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6c344-3da3-46d4-a0ea-3ac7cfcdd568",
   "metadata": {},
   "source": [
    "### Task 2: Generate Topics Using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8f649f-f80b-42f9-9796-07b0f6053f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Document-Term Matrix Creation Use  or  to create the matrix.\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Document-Term Matrix\n",
    "vectorizer = CountVectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d745edec-2076-4fdb-b1a1-af051eb52e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: evolving, technologies, healthcare, treatments, introduction\n",
      "Topic 2: critical, climate, change, digital, platforms\n",
      "Topic 3: stock, experiencing, due, future, industry\n",
      "Topic 4: investing, projects, energy, world, renewable\n",
      "Topic 5: intelligence, industries, artificial, revolutionized, worldwide\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Applying LDA Choose the number of topics (e.g., 5) and extract them.\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display Top Words for Each Topic\n",
    "def display_topics(model, feature_names, num_words):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx+1}: {', '.join([feature_names[i] for i in topic.argsort()[-num_words:]])}\")\n",
    "\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da8991-a3a2-4b8b-a059-7b1531cde0f5",
   "metadata": {},
   "source": [
    "1. **Documnet-Term Matrix Creation**\n",
    "* Preprocessed the text (lowercase conversion, removing special characters, stop words, and lemmatization).\n",
    "* Used tools like  or  to convert preprocessed text into a document-term matrix, which represents word frequencies in the documents.\n",
    "\n",
    "2. **Apply LDA(Latent Dirichlet Allocation)**\n",
    "* Chose the number of topics (e.g., 5) for analysis.\n",
    "* Used the LDA algorithm to identify underlying themes or topics in the dataset.\n",
    "\n",
    "3. **Display Top Words for Each Topic**\n",
    "* Extracted and display the top 5 words associated with each topic generated by the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b01d66-f09e-44fb-807e-d0f16b77d8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
